{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx \n",
    "import os, sys, joblib, time, multiprocessing\n",
    "from functools import reduce\n",
    "from IPython.display import clear_output\n",
    "from gprofiler.gprofiler import GProfiler\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "#For Graphing\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import hsv_to_rgb\n",
    "import random\n",
    "import networkx as nx\n",
    "import netwulf as wulf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd() + \"\\\\\"\n",
    "datadir = cwd + \"raw_data\\\\\"\n",
    "\n",
    "#Dataset from string\n",
    "ProteinInfo_path = \"https://stringdb-downloads.org/download/protein.info.v12.0/9606.protein.info.v12.0.txt.gz\"\n",
    "ProteinEnrichment_path = \"https://stringdb-downloads.org/download/protein.enrichment.terms.v12.0/9606.protein.enrichment.terms.v12.0.txt.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swapkeyval(indict):\n",
    "    \"\"\"\n",
    "    Takes in a dictionary and returns a swapped key-value dictionary\n",
    "    \"\"\"\n",
    "    outdict = dict()\n",
    "    for k, v in indict.items():\n",
    "        outdict[str(v)] = k\n",
    "    return outdict\n",
    "\n",
    "\n",
    "\n",
    "def CalcDensity(cluster):\n",
    "    \"\"\"\n",
    "    Calculate density of the entire graph\n",
    "    For a cluster C with n nodes and m edges, density is defined as:\n",
    "    Density(C) = 2m/n(n-1)\n",
    "    \"\"\"\n",
    "    num_edges = 0\n",
    "    num_nodes = len(cluster)\n",
    "    \n",
    "    # Count all edges\n",
    "    for node, neighbors in cluster.items():\n",
    "        num_edges += len(neighbors)\n",
    "    \n",
    "    num_edges //= 2\n",
    "    \n",
    "    if num_nodes < 2:\n",
    "        return 0\n",
    "    \n",
    "    density = (2 * num_edges) / (num_nodes * (num_nodes - 1))\n",
    "    return density\n",
    "\n",
    "\n",
    "\n",
    "def check_connection(vertices):\n",
    "    \"\"\"\n",
    "    Checks if the given vertices are all connected with every one of them.\n",
    "    Give two outputs:\n",
    "    - First output: returns True or False for the question \"is it fully connected?\"\n",
    "    - Second output: returns the groups with vertices that are connected between each other.\n",
    "    \"\"\"\n",
    "    # Get a list of all vertices in the graph\n",
    "    all_vertices = list(vertices.keys())\n",
    "    \n",
    "    # To keep track of visited vertices\n",
    "    visited = set()\n",
    "    \n",
    "    # List to store the connected components (groups)\n",
    "    components = []\n",
    "    \n",
    "    def bfs(start_vertex):\n",
    "        # Perform BFS starting from 'start_vertex'\n",
    "        visited.add(start_vertex)\n",
    "        queue = [start_vertex]\n",
    "        component = [start_vertex]\n",
    "        \n",
    "        while queue:\n",
    "            vertex = queue.pop(0)\n",
    "            for neighbor in vertices[vertex]:\n",
    "                if neighbor not in visited:\n",
    "                    visited.add(neighbor)\n",
    "                    queue.append(neighbor)\n",
    "                    component.append(neighbor)\n",
    "        return component\n",
    "    \n",
    "    # Iterate through all vertices and find all components\n",
    "    for vertex in all_vertices:\n",
    "        if vertex not in visited:\n",
    "            component = bfs(vertex)\n",
    "            components.append(component)\n",
    "    \n",
    "    return len(components) == 1, components    \n",
    "\n",
    "\n",
    "\n",
    "def count_occurances(counts_dict, edge_dict):\n",
    "    for key in edge_dict:\n",
    "        counts_dict[key] = counts_dict.get(key, 0) + edge_dict[key]\n",
    "    return counts_dict\n",
    "\n",
    "\n",
    "def lowest_first_from_to(u, v):\n",
    "    \"\"\"sorts the start_end by string value\"\"\"\n",
    "    return str(min([u,v])) + \"->\" + str(max([u,v]))\n",
    "\n",
    "def lowest_first_from_to_edge(u, v):\n",
    "    \"\"\"Sorts the edge name by string value\"\"\"\n",
    "    return str(min([u,v])) + \"-\" + str(max([u,v]))\n",
    "\n",
    "\n",
    "\n",
    "def girvan_newman_modularity(graph, clusters):\n",
    "    \"\"\"\n",
    "    Calculate modularity (Q) for a given interaction network and clusters, using Louvain Algorithm\n",
    "    \n",
    "    Parameters:\n",
    "    - graph: a dictionary. Keys are protein IDs, and values are dictionaries of neighbors and their weights.\n",
    "    - clusters: List of dictionaries, where each dictionary represents a cluster.\n",
    "                Keys are protein IDs, and values are dictionaries of neighbors and their weights.\n",
    "    \n",
    "    Returns:\n",
    "    - modularity (Q) value with range [-0.5, 1]\n",
    "    \"\"\"\n",
    "    total_weight = sum(\n",
    "        sum(neighbors.values()) for neighbors in graph.values()\n",
    "    ) / 2  \n",
    "\n",
    "    node_strength = {node: sum(neighbors.values()) for node, neighbors in graph.items()}\n",
    "\n",
    "    modularity = 0.0\n",
    "\n",
    "    for cluster in clusters:\n",
    "        nodes_in_cluster = set(cluster.keys())\n",
    "\n",
    "        in_cluster_weight = 0.0 \n",
    "        total_strength = sum(node_strength[node] for node in nodes_in_cluster)  \n",
    "\n",
    "        for node_i in nodes_in_cluster:\n",
    "            neighbors = graph.get(node_i, {})\n",
    "            for node_j, weight in neighbors.items():\n",
    "                if node_j in nodes_in_cluster:\n",
    "                    in_cluster_weight += weight\n",
    "\n",
    "        in_cluster_weight /= 2.0\n",
    "\n",
    "        \n",
    "        try:\n",
    "            modularity += (in_cluster_weight / total_weight) - (total_strength / (2 * total_weight)) ** 2\n",
    "        except:\n",
    "            print(total_weight, graph)\n",
    "            raise ValueError()\n",
    "\n",
    "\n",
    "    return modularity\n",
    "\n",
    "def enrichment_analysis(protein_list, organism = \"hsapiens\", sign_level = 0.05):\n",
    "    \n",
    "    # Initialize GProfiler object\n",
    "    print(protein_list)\n",
    "    gp = GProfiler(return_dataframe=True)\n",
    "\n",
    "    #Generate GProfiler with the information from protein sequences and organism\n",
    "    results = gp.profile(\n",
    "        organism=organism,\n",
    "        query=protein_list,\n",
    "        sources=['GO:BP', 'GO:MF', 'GO:CC', 'KEGG', 'REAC'],  # Include GO (BP, MF, CC), KEGG, and Reactome\n",
    "        significance_threshold_method='fdr'  # FDR for multiple testing correction\n",
    "    )\n",
    "    \n",
    "    if len(results) == 0:\n",
    "        return \"Missing more than one found protein\"\n",
    "\n",
    "    #Extract p-value from most significant term\n",
    "    pval = results[\"p_value\"].iloc[0]\n",
    "\n",
    "    #Return most significant term + p-value\n",
    "    if pval < sign_level:\n",
    "        return [results[\"name\"].iloc[0], results[\"p_value\"].iloc[0]]\n",
    "\n",
    "    else:\n",
    "        return \"No significant functions found\"\n",
    "\n",
    "def decode_edgesused(id : int):\n",
    "    \"\"\"\n",
    "    Decodes edges used integers to string interactions.\n",
    "    eg. 0 --> 0-1940 (not real interaction).\n",
    "\n",
    "    For decoding protein name, use \"decode_proteinname()\"\n",
    "    \"\"\"\n",
    "\n",
    "def decode_proteinname():\n",
    "    \"\"\"\n",
    "    Decodes protein used integers to string names as given in stringDB.\n",
    "    eg. 0 --> ENV09918398 (not real protein).\n",
    "\n",
    "    For decoding edges in cluster, use \"decode_edgesused()\"\n",
    "    \"\"\"\n",
    "\n",
    "def construct_graph(input, graph_name : str = \"PPI_GraphNetwork\", reformat : bool = False, debug_mode : bool = False):\n",
    "        \"\"\"\n",
    "        Initializes graph network and constructs edges, based on input data.\n",
    "        Input data should be in the format: dict(dict()), where the inner and outer key is a protein id, and the inner value is the normalized combined score.\n",
    "        Example: interaction_data[0][9827] = 0.3; the interaction between protein 0 and 9827, has (normalized) probability = 0.3\n",
    "        *reformat [WIP]*  = True: converts input data to appropriate netwulf object for nx.visualize() - Running reformat_clustervar(). Else, iterates through input adding edges and nodes one at a time\n",
    "        \"\"\"\n",
    "        graph_network = nx.Graph(name=graph_name) #initialize empty graph\n",
    "        if debug_mode: print(\"##### Splitting label and cluster dict #####\")\n",
    "        enrichment_labels = []\n",
    "        data = []\n",
    "        for entry in input:\n",
    "            if debug_mode: print(entry)\n",
    "            enrichment_labels.append(entry[0])\n",
    "            data.append(entry[1])\n",
    "\n",
    "        #Creating nodes in network\n",
    "        if debug_mode: print(\"##### Adding nodes to graph #####\")\n",
    "        allproteins = set()\n",
    "        for cluster in data:\n",
    "            for root in cluster.keys():\n",
    "                allproteins.add(root)\n",
    "                for neighbor in cluster.keys():\n",
    "                    allproteins.add(neighbor)\n",
    "                    graph_network.add_node(neighbor, size = np.random.random())\n",
    "                    if debug_mode: print(f\"--- Node collected: {neighbor} ---\")\n",
    "        #graph_network.add_nodes_from(allproteins)\n",
    "\n",
    "        #Simple adding of edges to graph network - Asbjørn\n",
    "        if debug_mode: print(\"##### Adding edges to graph #####\")\n",
    "        for cluster in data:\n",
    "            for root in cluster.keys():\n",
    "                for neighbor in cluster[root].keys():\n",
    "                    if root != neighbor:\n",
    "                        graph_network.add_edge(root, neighbor)\n",
    "                        if debug_mode: print(f\"--- Edge added between: {root, neighbor} ---\")\n",
    "\n",
    "        return graph_network\n",
    "\n",
    "def reformat_clustervar(data):\n",
    "    \"\"\"\n",
    "    NB: not developed yet!\n",
    "    Converts input data to appropriate netwulf object for nx.visualize()\n",
    "    \"\"\"\n",
    "\n",
    "def custom_pool(job, target, ProgressBar : bool = True):\n",
    "    \"\"\"\n",
    "    Keeps track of child processes in map reduce when multiprocessing.pool is used\n",
    "    *job*: custom task to be performed\n",
    "    *target*: target to perform task on\n",
    "    \"\"\"\n",
    "    #Logfile for tracking - in dir <CustomPool_logs>\n",
    "    #var init\n",
    "    collected_results = []\n",
    "    total_iters = len(target)\n",
    "    \n",
    "    #Run with tqdm()\n",
    "    if ProgressBar: \n",
    "        with multiprocessing.Pool() as pool:\n",
    "            for result in tqdm(pool.imap_unordered(job, target), total=len(target), desc=\"Child processes completed:\"):\n",
    "                collected_results.append(result)\n",
    "    \n",
    "    #Run without tqdm - Progress saved to CustomPool_logs\n",
    "    else: \n",
    "        with multiprocessing.Pool() as pool:\n",
    "            iter = pool.imap_unordered(job, target)\n",
    "            \n",
    "            while True:\n",
    "                with open(f\"./CustomPool_logs/CompletedContent.txt\", \"w\") as logfile: #each ChildProcess writes to own logfile\n",
    "                    try:\n",
    "                        result = next(iter)\n",
    "                    \n",
    "                    except StopIteration:                            \n",
    "                    # All jobs have been processed\n",
    "                        print(\"All child jobs completed.\")\n",
    "                        break\n",
    "                        \n",
    "                    except ChildProcessError as err:\n",
    "                        with open(\"./CustomPool_logs/FailedPools.txt\", \"w\") as logfile_fail:\n",
    "                            print(f\"Processing of {err.args[0]} job failed.\", file=logfile_fail)\n",
    "                        \n",
    "                    else:\n",
    "                        collected_results.append(result)\n",
    "                        with open(f\"./CustomPool_logs/Pool_Iteration_Completed.txt\", \"w\") as iterfile:\n",
    "                            print(f\"Job {len(collected_results)} completed!\", file=iterfile)\n",
    "                        print(f\"Job {len(collected_results)} completed! \\n{result}\", file=logfile)\n",
    "\n",
    "    if collected_results:\n",
    "        with open(\"./CustomPool_logs/AllPool_logs.txt\", \"w\") as logfile:\n",
    "            print('All completed jobs:', collected_results, file=logfile)\n",
    "    \n",
    "    return collected_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class interaction_network:\n",
    "    \"\"\"**Interaction network:**\\n\n",
    "    This class object is used to keep track of both vertices and edges in a connected network.\n",
    "    Furthermore, it contains functions for creating clusters from raw data\n",
    "    The class can be initialized with arguments for concurrent changes to several functions\\n\\n\n",
    "    **Class functions:**\\n\n",
    "    load_data(data_path, filename): Loads network data from a tsv file\\n\\n\n",
    "    **Class attributes:**\\n\n",
    "    self.vertices: a dictonary where the keys are the protein names, and the value is a list of protein names of interacting proteins\\n\n",
    "    self.testdataset: applies a smaller test data from string (not human proteins)\\n\n",
    "    self.threshold: given threshold in percent TO REMOVE - meaning threshold=0.05 removes lowest 5% values of combined_score. Must be between 0 and 1\n",
    "    self.NewPython3912: NewPython3912 is given as true, if user is using python version 3.9.12 or older (in that case, tqdm() parsing of data wont run and progress bar will not appear!)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 testdataset : bool = False,\n",
    "                 threshold : int = 0,\n",
    "                 NewPython3912 : bool = False):\n",
    "        self.vertices = dict()\n",
    "        self.data = None\n",
    "        self.encoding_dict = None\n",
    "        self.encoding_edgesused = {}\n",
    "        self.occurances_small = None\n",
    "        self.graph_name = \"PPI_GraphNetwork\"\n",
    "        self.graph_network = None\n",
    "        self.shortest_paths = dict()\n",
    "        self.testdataset = testdataset\n",
    "        self.threshold = threshold\n",
    "        self.NewPython3912 = NewPython3912\n",
    "        self.finished_clusters = list()\n",
    "        self.CollectMostTravelled = list() #Used for node size in graphing\n",
    "        self.CurrentSeveranceScore = None #Used for node size in graphing\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"\\n\".join(f\"{key} {value}\" for key, value in self.vertices.items())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"\\n\".join(f\"{key} {value}\" for key, value in self.vertices.items())\n",
    "    \n",
    "\n",
    "    def create_encoding_dict(self, \n",
    "                             file_url: str = \"https://stringdb-downloads.org/download/protein.info.v12.0/9606.protein.info.v12.0.txt.gz\", \n",
    "                             compression : str = \"gzip\", \n",
    "                             sep : str = \"\\t\"):\n",
    "        if self.testdataset:\n",
    "            file_url = \"https://stringdb-downloads.org/download/protein.info.v12.0/9606.protein.info.v12.0.txt.gz\"\n",
    "        \n",
    "        #Message\n",
    "        print(\"Creating encoding table\")\n",
    "\n",
    "        #Loading the relevant data\n",
    "        self.encoding_dict = pd.read_csv(file_url, compression=compression, sep=sep)\n",
    "\n",
    "        #Isolating the string id\n",
    "        self.encoding_dict = self.encoding_dict[[\"#string_protein_id\"]].values\n",
    "\n",
    "        #Converting to a dictionary\n",
    "        self.encoding_dict = {str(c):e[0] for c, e in enumerate(self.encoding_dict)}\n",
    "\n",
    "        #Swapping keys and values\n",
    "        self.encoding_dict = swapkeyval(self.encoding_dict)\n",
    "\n",
    "\n",
    "    def load_data(self,\n",
    "                  file_url: str = \"https://stringdb-downloads.org/download/protein.links.detailed.v12.0/9606.protein.links.detailed.v12.0.txt.gz\", \n",
    "                  compression : str = \"gzip\", \n",
    "                  sep : str = \" \", \n",
    "                  req_experimental : bool = True):\n",
    "        #input control threshold\n",
    "        if self.threshold > 100 or self.threshold < 0:\n",
    "            raise ValueError(\"Give threshold in percent, between 0 and 100%\")\n",
    "\n",
    "        #Test dataset\n",
    "        if self.testdataset:\n",
    "            file_url = \"https://stringdb-downloads.org/download/stream/protein.links.detailed.v12.0/9606.protein.links.detailed.v12.0.onlyAB.txt.gz\"\n",
    "\n",
    "        #Checking if an encoding dict have been created:\n",
    "        if not self.encoding_dict:\n",
    "            raise SyntaxError(\"self.create_encoding_dict() should be run prior to load_data\")\n",
    "\n",
    "        #Loading the data\n",
    "        print(\"Fetching data\")\n",
    "        self.data = pd.read_csv(file_url, compression=compression, sep=sep)\n",
    "\n",
    "        #If we only want interactions with experimental evidence:\n",
    "        if req_experimental:\n",
    "            self.data = self.data[self.data[\"experimental\"] > 0]\n",
    "        \n",
    "\n",
    "        #Encoding the data frame:\n",
    "        print(\"Cropping data\")\n",
    "        self.data = self.data.apply(lambda series: series.map( lambda x: self.encoding_dict[x] if x in self.encoding_dict else x)).reset_index()\n",
    "        self.data = self.data[[\"protein1\", \"protein2\", \"combined_score\"]]\n",
    "        threshold_value = self.data[\"combined_score\"].quantile(self.threshold) #calculate combined_score value associated with given threshold percentage\n",
    "        self.data = self.data[self.data[\"combined_score\"] > threshold_value] #filter threshold\n",
    "        \n",
    "        #Return info on threshold value\n",
    "        if threshold_value > 0: \n",
    "            print(f\"Given threshold: {self.threshold} filtered out combined_score values: {threshold_value} and below\")\n",
    "        else:\n",
    "            print(f\"Given threshold: {self.threshold} filtered out no values\")\n",
    "        \n",
    "        #Min-max normalizing of \"combined_score\"\n",
    "        self.data[\"combined_score\"] = (self.data[\"combined_score\"]-self.data[\"combined_score\"].min())/(self.data[\"combined_score\"].max()-self.data[\"combined_score\"].min()) if self.data[\"combined_score\"].min() != self.data[\"combined_score\"].max() else self.data[\"combined_score\"] / self.data[\"combined_score\"].max()\n",
    "        self.data[\"combined_score\"] = self.data[\"combined_score\"].round(2)\n",
    "\n",
    "\n",
    "        #Flag for NewPython3912 (tqdm() wont run in newer versions)\n",
    "        if self.NewPython3912:\n",
    "            print(\"Parsing data without progress bar...\")\n",
    "            for i in range(len(self.data)):\n",
    "                row = self.data.iloc[i]\n",
    "                try:\n",
    "                    linedata = [row[\"protein1\"], row[\"protein2\"], row[\"combined_score\"]]\n",
    "                    # Check if the main key exists; if not, initialize it as an empty dictionary\n",
    "                    if int(linedata[0]) not in self.vertices:\n",
    "                        self.vertices[int(linedata[0])] = {}\n",
    "                    if int(linedata[1]) not in self.vertices:\n",
    "                        self.vertices[int(linedata[1])] = {}\n",
    "                    \n",
    "                    # Update the sub-dictionary with the new key-value pair\n",
    "                    self.vertices[int(linedata[0])][int(linedata[1])] = linedata[2]\n",
    "                    self.vertices[int(linedata[1])][int(linedata[0])] = linedata[2]\n",
    "\n",
    "                except:\n",
    "                    raise ValueError(f\"Row {row} was discarded\")\n",
    "        \n",
    "        else: #in case python version is \"old\" and tqdm() can run   \n",
    "            for i in tqdm(range(len(self.data)), desc=\"Parsing data\"):\n",
    "                row = self.data.iloc[i]\n",
    "                try:\n",
    "                    linedata = [row[\"protein1\"], row[\"protein2\"], row[\"combined_score\"]]\n",
    "                    # Check if the main key exists; if not, initialize it as an empty dictionary\n",
    "                    if int(linedata[0]) not in self.vertices:\n",
    "                        self.vertices[int(linedata[0])] = {}\n",
    "                    if int(linedata[1]) not in self.vertices:\n",
    "                        self.vertices[int(linedata[1])] = {}\n",
    "                    \n",
    "                    # Update the sub-dictionary with the new key-value pair\n",
    "                    self.vertices[int(linedata[0])][int(linedata[1])] = linedata[2]\n",
    "                    self.vertices[int(linedata[1])][int(linedata[0])] = linedata[2]\n",
    "\n",
    "                except:\n",
    "                    raise ValueError(f\"Row {row} was discarded\")\n",
    "        \n",
    "        #Deleting the data. We don't need it anymore, since we have loaded\n",
    "        self.data = None\n",
    "\n",
    "        #Putting the self.vertices item into a list. These will be the preliminary clusters\n",
    "        self.vertices = [self.vertices]\n",
    "\n",
    "        #Check connectivity of the cluster\n",
    "        is_connected, connected_keys = check_connection(self.vertices[0])\n",
    "\n",
    "        #If not all vertices are interconnected, we split the cluster\n",
    "        if not is_connected:\n",
    "            self.split_cluster(0, connected_keys)\n",
    "\n",
    "        #Printing the size of the data structure\n",
    "        print(f\"Data loaded in var .vertices, with size: {sys.getsizeof(self.vertices)/1000000}mb\")\n",
    "\n",
    "\n",
    "    def construct_graph(self,\n",
    "                        interaction_data: dict):\n",
    "        \"\"\"\n",
    "        Initializes graph network and constructs edges, based on input data.\n",
    "        Input data should be in the format: dict(dict()), where the inner and outer key is a protein id, and the inner value is the normalized combined score.\n",
    "        Example: interaction_data[0][9827] = 0.3; the interaction between protein 0 and 9827, has (normalized) probability = 0.3\n",
    "        \"\"\"\n",
    "        self.graph_network = nx.Graph(name=self.graph_name) #initialize empty graph\n",
    "        \n",
    "        #Creating nodes in network\n",
    "        allproteins = set()\n",
    "        for root in interaction_data.keys():\n",
    "            allproteins.add(root)\n",
    "            for neighbor in interaction_data.keys():\n",
    "                allproteins.add(neighbor)\n",
    "        self.graph_network.add_nodes_from(allproteins)\n",
    "\n",
    "        #Simple adding of edges to graph network - Asbjørn\n",
    "        for root in interaction_data.keys():\n",
    "            for neighbor in interaction_data[root].keys():\n",
    "                if root != neighbor:\n",
    "                    self.graph_network.add_edge(root, neighbor)\n",
    "    \n",
    "\n",
    "    def shortest_path(self, vertex, debug_mode=False, method=\"dijkstra\"):\n",
    "        # Setting up a dictionary of shortest paths and the start vertex is put in a list\n",
    "        shortest_paths = dict()\n",
    "        to_process = [str(vertex)]\n",
    "\n",
    "        if method.lower() == \"bfs\":\n",
    "            raise NotImplementedError\n",
    "\n",
    "            # While the \"to_process\" list is not empty, we do branch and bound\n",
    "            while to_process:\n",
    "                if debug_mode:\n",
    "                    print(len(to_process), len(shortest_paths))\n",
    "\n",
    "                #Taking one of the short branches to process\n",
    "                current_branch = to_process.pop(0)\n",
    "\n",
    "                # We look through all neighbors. If it goes to a vertex already in the path, it's unoptimal and is discarded\n",
    "                for neighbor in self.vertices[int(current_branch.split(\"_\")[-1])]:\n",
    "\n",
    "                    # If the path doesn't loop, we add the neighbor to the current branch and check if it's a new shortest path\n",
    "                    new_path = current_branch + \"_\" + str(neighbor)\n",
    "                    start_end = new_path.split(\"_\")[0] + \"->\" + str(neighbor)\n",
    "\n",
    "                    if start_end in shortest_paths:\n",
    "                        # If there are more paths of equal length, we're working with a list\n",
    "                        if isinstance(shortest_paths[start_end], list):\n",
    "                            if len(new_path.split(\"_\")) < len(shortest_paths[start_end][0].split(\"_\")):\n",
    "                                shortest_paths[start_end] = new_path\n",
    "                                to_process.append(new_path)\n",
    "                            #elif len(new_path.split(\"_\")) == len(shortest_paths[start_end][0].split(\"_\")):\n",
    "                            #    shortest_paths[start_end].append(new_path)\n",
    "                            #    to_process.append(new_path)\n",
    "                        else:\n",
    "                            # If there's only one shortest path found until now\n",
    "                            if len(new_path.split(\"_\")) < len(shortest_paths[start_end].split(\"_\")):\n",
    "                                shortest_paths[start_end] = new_path\n",
    "                                to_process.append(new_path)\n",
    "                            #elif len(new_path.split(\"_\")) == len(shortest_paths[start_end].split(\"_\")):\n",
    "                            #    shortest_paths[start_end] = [shortest_paths[start_end], new_path]\n",
    "                            #    to_process.append(new_path)\n",
    "                    else:\n",
    "                        # If no shortest path has been identified between the two points\n",
    "                        shortest_paths[start_end] = new_path\n",
    "                        to_process.append(new_path)\n",
    "\n",
    "            return shortest_paths\n",
    "\n",
    "        elif method.lower() == \"dijkstra\":\n",
    "            #Adding an initial distance of 0\n",
    "            to_process = [[str(vertex), 0]]\n",
    "\n",
    "            # While the \"to_process\" list is not empty, we do branch and bound\n",
    "            while to_process:\n",
    "                if debug_mode:\n",
    "                    print(len(to_process), len(self.shortest_paths))\n",
    "\n",
    "                #Taking one of the short branches to process\n",
    "                current_branch = to_process.pop(0)\n",
    "\n",
    "                if debug_mode:\n",
    "                    print(current_branch[0])\n",
    "\n",
    "                # We look through all neighbors. If it goes to a vertex already in the path, it's unoptimal and is discarded\n",
    "                for neighbor in self.vertices[0][int(current_branch[0].split(\"_\")[-1])]:\n",
    "                    if str(neighbor) in current_branch[0].split(\"_\"):\n",
    "                        continue\n",
    "\n",
    "                    # If the path doesn't loop, we add the neighbor to the current branch and check if it's a new shortest path\n",
    "                    #Adding the neigbor\n",
    "                    new_path = [current_branch[0] + \"_\" + str(neighbor), current_branch[1]]\n",
    "                    start_end = lowest_first_from_to(new_path[0].split(\"_\")[0], str(neighbor))\n",
    "                    new_path_split = new_path[0].split(\"_\")\n",
    "                    new_path[1] = new_path[1] + (1 - self.vertices[0][int(new_path_split[-2])][int(new_path_split[-1])])\n",
    "\n",
    "\n",
    "\n",
    "                    if start_end in self.shortest_paths:\n",
    "                        if new_path[1] < self.shortest_paths[start_end][1]:\n",
    "                            self.shortest_paths[start_end] = new_path\n",
    "                            to_process.append(new_path)\n",
    "                    else:\n",
    "                        # If no shortest path has been identified between the two points\n",
    "                        self.shortest_paths[start_end] = new_path\n",
    "                        to_process.append(new_path)\n",
    "\n",
    "            #Returning a list of all edges A-B\n",
    "            edges_dict = dict()\n",
    "            for item in self.shortest_paths:\n",
    "                for i in range(1, len(split_path := self.shortest_paths[item][0].split(\"_\"))):\n",
    "                    edge = lowest_first_from_to_edge(split_path[i-1], split_path[i])\n",
    "                    edges_dict[edge] = edges_dict.get(edge, 0) + 1\n",
    "            #num_included = 1000\n",
    "            #keys_to_include = [item[0] for item in sorted(edges_dict.items(), key = lambda x: x[1], reverse=True)[:num_included]]\n",
    "            return edges_dict #{key:edges_dict[key] for key in keys_to_include}\n",
    "\n",
    "\n",
    "    def evaluate_most_used_path(self, cluster, debug_mode=False):\n",
    "        \"\"\"This function uses mapreduce to count the number of times an edge is in a shortest path. \n",
    "        The function returns a dictionary where they keys are edges and the value is counts\"\"\"\n",
    "        if debug_mode:\n",
    "            print(\"Initializing MapReduce of shortest_path()...\") #debug_mode\n",
    "            counter = 0\n",
    "            len_cluster = len(cluster)\n",
    "            for vertex in cluster:\n",
    "                counter += 1\n",
    "                print(f\"eval_most_used_path_progress = {counter}/{len_cluster}\")\n",
    "                results = self.shortest_path(vertex)\n",
    "        else:\n",
    "            #Mapping the shortest_path function onto the cluster\n",
    "            print(\"-----Mapping-----\")\n",
    "            results = custom_pool(self.shortest_path, cluster, ProgressBar=True)\n",
    "\n",
    "            #Reducing the result by using count_occurances\n",
    "            print(\"-----Reducing-----\")\n",
    "            occurances = reduce(count_occurances, results, {})\n",
    "\n",
    "        #tweaking data structure with encoding dict. This was not needed\n",
    "        #if debug_mode: print(\"Initializing MapReduce of shortest_path()...\") #debug_mode\n",
    "        #occurances_small = {}\n",
    "        #count=0\n",
    "        #if self.NewPython3912: #New python - no progress bar\n",
    "        #    print(\"Creating encoding dict without progress bar...\") #debug_mode\n",
    "        #    for key, value in occurances.items():\n",
    "        #        self.encoding_edgesused[count] = key\n",
    "        #        occurances_small[count] = value\n",
    "        #        count += 1\n",
    "        #else: #Old python - with progress bar\n",
    "        #    for key, value in tqdm(occurances.items(), desc=\"Parsing encoding_edgesused data\"):\n",
    "        #        self.encoding_edgesused[count] = key\n",
    "        #        occurances_small[count] = value\n",
    "        #        count += 1\n",
    "\n",
    "        #self.occurances_small = occurances_small\n",
    "        return occurances# occurances_small\n",
    "    \n",
    "\n",
    "    def severance_score(self, shortest_paths):\n",
    "        \"\"\"Calculating the severance score by dividing by the weight. The weak edges will be preferred\"\"\"\n",
    "        for key in shortest_paths:\n",
    "            start_end = key.split(\"-\")\n",
    "            shortest_paths[key] = shortest_paths[key] / self.vertices[0][int(start_end[0])][int(start_end[1])]\n",
    "        return shortest_paths\n",
    "    \n",
    "    def find_edge_to_cut(self, severance_scores):\n",
    "        edge_to_cut = sorted(severance_scores.items(), key = lambda x:x[1], reverse=True)[0][0]\n",
    "        return edge_to_cut\n",
    "\n",
    "    def cut_edge(self, clusternumber, edge_to_cut):\n",
    "        start_end = edge_to_cut.split(\"-\")\n",
    "        path_format = \"_\".join(start_end)\n",
    "\n",
    "        #Deleting the edge from the data\n",
    "        del self.vertices[clusternumber][int(start_end[0])][int(start_end[1])]\n",
    "        del self.vertices[clusternumber][int(start_end[1])][int(start_end[0])]\n",
    "\n",
    "        #Deleting shortest paths that rely on this edge.\n",
    "        for key in self.shortest_paths:\n",
    "            if path_format in self.shortest_paths[key]:\n",
    "                del self.shortest_paths[key]\n",
    "    \n",
    "    def split_cluster(self, clusterindex, list_of_keylists):\n",
    "        \"\"\"Splitting the cluster using a list of lists of keys that should be grouped together\"\"\"\n",
    "        new_clusters = list()\n",
    "        for keylist in list_of_keylists:\n",
    "            new_clusters.append(dict())\n",
    "            for key in keylist:\n",
    "                new_clusters[-1][key] = self.vertices[clusterindex][key]\n",
    "        del self.vertices[clusterindex]\n",
    "        for new_cluster in new_clusters:\n",
    "            self.vertices.append(new_cluster)\n",
    "\n",
    "    def edge_to_remove(self):\n",
    "        \"\"\"Function for determining which edge should be removed from the cluster\"\"\"\n",
    "        print(\"-----Finding edge to remove-----\")\n",
    "        res = self.evaluate_most_used_path(self.vertices[0])\n",
    "        res = self.severance_score(res)\n",
    "        res = self.find_edge_to_cut(res)\n",
    "        return res\n",
    "\n",
    "    def cluster(self):\n",
    "        #While there are still clusters to be processed, we run a loop of cutting and evaluating\n",
    "        edge_to_remove = \"\"\n",
    "        while len(self.vertices) > 0:\n",
    "            print(f\"NEW ITERATION --- Clusters left: {len(self.vertices)} --- Cluster length: {len(self.vertices[0])} --- Finished clusters: {len(self.finished_clusters)} --- Latest cluster length: {len(self.finished_clusters[-1][1].values()) if self.finished_clusters else 0} --- last edge severed: {edge_to_remove}\")\n",
    "            \n",
    "            #Determine density of the cluster\n",
    "            print(\"-----Density calculation-----\")\n",
    "            density = CalcDensity(self.vertices[0])\n",
    "            #print(f\"-----Density: {density}-----\")\n",
    "\n",
    "            #If the conditions are met, we classify by GSEA\n",
    "            if density >= 0.7:\n",
    "                print(\"-----GSEA-----\")\n",
    "                decoded_cluster = self.vertices[0] #self.decode_edgesused(cluster=self.vertices[0])\n",
    "                self.finished_clusters.append([enrichment_analysis(list(decoded_cluster.keys())), decoded_cluster])\n",
    "                del self.vertices[0]\n",
    "\n",
    "            #If it does not satisfy the density condition, we find an edge to cut\n",
    "            else:\n",
    "                print(\"-----Evaluate edge removal-----\")\n",
    "                copy_current_cluster = self.vertices[0].copy()   #Making a copy of the current cluster so we can revert the cut\n",
    "                edge_to_remove = self.edge_to_remove()   #Identifying which edge to remove\n",
    "                self.cut_edge(0, edge_to_remove) #Removing the edge\n",
    "\n",
    "                #Check connectivity of the cluster\n",
    "                is_connected, connected_keys = check_connection(self.vertices[0])\n",
    "\n",
    "                #If not all vertices are interconnected, we split the cluster\n",
    "                if not is_connected:\n",
    "                    self.split_cluster(0, connected_keys)\n",
    "\n",
    "                    #modularity calculation:\n",
    "                    print(\"-----Modularity-----\")\n",
    "                    modularity = girvan_newman_modularity(copy_current_cluster, self.vertices[-len(connected_keys):])\n",
    "\n",
    "                    #If the modularity will not be increased with the split, we restore the edge and finalize the cluster\n",
    "                    if modularity < 0:\n",
    "                        print(\"-----Finalized previous cluster-----\")\n",
    "                        self.CollectMostTravelled.append(self.CurrentSeveranceScore) #Collect severance scores if cluster is completed - used in graphing\n",
    "                        decoded_cluster = copy_current_cluster #self.decode_edgesused(cluster=copy_current_cluster)\n",
    "                        self.finished_clusters.append([enrichment_analysis(list(decoded_cluster.keys())), decoded_cluster])\n",
    "                        self.vertices = self.vertices[:-3]\n",
    "                    \n",
    "                    #If the modularity is increased, we finalize the split\n",
    "                    else:\n",
    "                        print(\"-----Finalized split-----\")\n",
    "\n",
    "\n",
    "            clear_output(wait = False) #Clear output after each iteration\n",
    "                        \n",
    "\n",
    "        \n",
    "        self.write_clusters()\n",
    "\n",
    "    def write_clusters(self):\n",
    "        joblib.dump(self.finished_clusters, \"./joblib_vars/finished_clusters.joblib.gz\")\n",
    "        #outfile = open(\"results.txt\", \"w\")\n",
    "        #for finished_cluster in self.finished_clusters:\n",
    "        #    outfile.write(finished_cluster[0] + \"\\n\")\n",
    "        #    for key in finished_cluster[1]:\n",
    "        #        outfile.write(key + \": \" + \", \".join(finished_cluster[1][key]))\n",
    "        #outfile.close()\n",
    "\n",
    "    def decode_edgesused(self, id : int = 0, cluster : dict = None, full_dict : bool = False):\n",
    "        \"\"\"\n",
    "        Decodes edges used integers to string interactions.\n",
    "        eg. 0 --> 0-1940 (not real interaction).\n",
    "\n",
    "        cluster = insert cluster ; returns a decoded cluster in case cluster is given        \n",
    "        full_dict = True ; Return full dictionary, not just interaction for id given\n",
    "\n",
    "        For decoding protein name, use \"decode_proteinname()\"\n",
    "        \"\"\"\n",
    "        return_dict = {}\n",
    "        if full_dict: #return full dictionary, not just interaction for id given\n",
    "            for key, value in self.occurances_small.items():\n",
    "                return_dict[self.encoding_edgesused[key]] = value\n",
    "            return return_dict\n",
    "        \n",
    "        elif cluster != None:\n",
    "            for key, value in cluster.items():\n",
    "                return_dict[self.encoding_edgesused[key]] = value\n",
    "            return return_dict\n",
    "\n",
    "        else:\n",
    "            return self.encoding_edgesused[key]\n",
    "\n",
    "    def decode_proteinname(self, id : int = 0, cluster : dict = None, full_dict : bool = False):\n",
    "        \"\"\"\n",
    "        Decodes protein used integers to string names as given in stringDB.\n",
    "        eg. 0 --> ENV09918398 (not real protein).\n",
    "\n",
    "        cluster = insert cluster ; returns a decoded cluster in case cluster is given \n",
    "        full_dict = True ;Return full dictionary, not just protein name for id given\n",
    "\n",
    "        For decoding edges in cluster, use \"decode_edgesused()\"\n",
    "        \"\"\"\n",
    "        return_dict = {}\n",
    "        if full_dict: #return full dictionary, not just protein name for id given\n",
    "            for key, value in self.occurances_small.items():\n",
    "                return_dict[self.encoding_dict[key]] = value\n",
    "            return return_dict\n",
    "\n",
    "        elif cluster != None:\n",
    "            for key, value in cluster.items():\n",
    "                return_dict[self.encoding_edgesused[key]] = value\n",
    "            return return_dict\n",
    "\n",
    "        else:\n",
    "            return self.encoding_dict[key]\n",
    "\n",
    "    def construct_graph(self, input, graph_name : str = \"PPI_GraphNetwork\", reformat : bool = False, debug_mode : bool = False):\n",
    "        \"\"\"\n",
    "        Initializes graph network and constructs edges, based on input data.\n",
    "        Input data should be in the format: dict(dict()), where the inner and outer key is a protein id, and the inner value is the normalized combined score.\n",
    "        Example: interaction_data[0][9827] = 0.3; the interaction between protein 0 and 9827, has (normalized) probability = 0.3\n",
    "        *reformat [WIP]*  = True: converts input data to appropriate netwulf object for nx.visualize() - Running reformat_clustervar(). Else, iterates through input adding edges and nodes one at a time\n",
    "        \"\"\"\n",
    "        self.graph_network = nx.Graph(name=self.graph_name) #initialize empty graph\n",
    "        if debug_mode: print(\"##### Splitting label and cluster dict #####\")\n",
    "        enrichment_labels = []\n",
    "        data = []\n",
    "        for entry in input:\n",
    "            if debug_mode: print(entry)\n",
    "            enrichment_labels.append(entry[0])\n",
    "            data.append(entry[1])\n",
    "\n",
    "        #Creating nodes in network\n",
    "        if debug_mode: print(\"##### Adding nodes to graph #####\")\n",
    "        allproteins = set()\n",
    "        for cluster in data:\n",
    "            for root in cluster.keys():\n",
    "                allproteins.add(root)\n",
    "                for neighbor in cluster.keys():\n",
    "                    allproteins.add(neighbor)\n",
    "                    self.graph_network.add_node(neighbor, size = self.NodeSize(node=neighbor)) #Node size = sum of normalized combined score\n",
    "                    if debug_mode: print(f\"--- Node collected: {neighbor} ---\")\n",
    "        #graph_network.add_nodes_from(allproteins)\n",
    "\n",
    "        #Simple adding of edges to graph network - Asbjørn\n",
    "        if debug_mode: print(\"##### Adding edges to graph #####\")\n",
    "        for cluster in data:\n",
    "            for root in cluster.keys():\n",
    "                for neighbor in cluster[root].keys():\n",
    "                    if root != neighbor:\n",
    "                        self.graph_network.add_edge(root, neighbor)\n",
    "                        if debug_mode: print(f\"--- Edge added between: {root, neighbor} ---\")\n",
    "\n",
    "\n",
    "    def NodeSize(self, node):\n",
    "        \"\"\"\n",
    "        Finding node size from sum of normalized combined score, using self.data\n",
    "        \"\"\"\n",
    "        sum_NormCombinedScore = 0\n",
    "        for prot_col in [\"protein1\", \"protein2\"]:\n",
    "            curr_data = self.data[self.data[prot_col] == node]\n",
    "            sum_NormCombinedScore += curr_data[\"combined_score\"].sum()\n",
    "        return sum_NormCombinedScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating encoding table\n",
      "Fetching data\n",
      "Cropping data\n",
      "Given threshold: 0.995 filtered out combined_score values: 998.0 and below\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c5b4fc0fa744e50b88a88ce6b8a5b78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing data:   0%|          | 0/13761 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded in var .vertices, with size: 0.004216mb\n"
     ]
    }
   ],
   "source": [
    "#Creating the network object\n",
    "network = interaction_network(testdataset = True, threshold=0.995)\n",
    "\n",
    "#Creating an encoding dict for the data to save memory\n",
    "network.create_encoding_dict()\n",
    "\n",
    "#Loading the data and formatting it\n",
    "network.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW ITERATION --- Clusters left: 509 --- Cluster length: 2808 --- Finished clusters: 0 --- Latest cluster length: 0 --- last edge severed: \n",
      "-----Density calculation-----\n",
      "-----Evaluate edge removal-----\n",
      "-----Finding edge to remove-----\n",
      "-----Mapping-----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2387b50a82d4f3c989f95ec97299d8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Child processes completed::   0%|          | 0/2808 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "network.cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting cluster data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "edges = []\n",
    "\n",
    "counter = 0\n",
    "for cluster in network.finished_clusters:\n",
    "    if counter < 10:\n",
    "        if len(cluster[1].values()) >= 0:\n",
    "            #Extract the cluster labels\n",
    "            if len(cluster[0]) == 2:\n",
    "                labels.append([cluster[0][0], float(cluster[0][1])])\n",
    "            else:\n",
    "                labels.append(cluster[0])\n",
    "            #If needed resets weights to 1\n",
    "            for value in cluster[1].values():\n",
    "                for key in value:\n",
    "                    value[key] = 1\n",
    "            \n",
    "            edges.append(cluster[1])\n",
    "\n",
    "            counter += 1\n",
    "\n",
    "cluster_data = []\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    assert len(labels) == len(edges)\n",
    "    cluster_data.append([labels[i], edges[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matplotlib Cluster Graph (Clusters with labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "\n",
    "# Assign a color for each cluster\n",
    "cluster_colors = {}\n",
    "p_values = []\n",
    "for i, cluster in enumerate(cluster_data):\n",
    "    labels, edges = cluster\n",
    "    if len(labels[0]) > 1:\n",
    "        label = labels[0]\n",
    "        p_values.append(labels[1])\n",
    "    else:\n",
    "        label = labels\n",
    "        p_values.append(1)\n",
    "    label = f\"{label}_{i}\"  # Add unique suffix\n",
    "    color = hsv_to_rgb((random.random(), 0.8, 0.8))  # Random bright color\n",
    "    cluster_colors[label] = color\n",
    "    \n",
    "    # Add nodes and edges to the graph\n",
    "    for node, connections in edges.items():\n",
    "        G.add_node(node, cluster=label)  # Add node with cluster label\n",
    "        for neighbor, weight in connections.items():\n",
    "            G.add_edge(node, neighbor, weight=weight)\n",
    "\n",
    "# Draw the graph\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Get node positions using a layout algorithm\n",
    "pos = nx.spring_layout(G)\n",
    "\n",
    "# Draw nodes and edges\n",
    "nx.draw_networkx_edges(G, pos)\n",
    "nx.draw_networkx_labels(G, pos, font_size=10, font_color=\"black\")\n",
    "\n",
    "counter = 0\n",
    "# Draw nodes with colors based on cluster\n",
    "for cluster, color in cluster_colors.items():\n",
    "    nodes_in_cluster = [n for n, attr in G.nodes(data=True) if attr['cluster'] == cluster]\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=nodes_in_cluster, node_color=[color], label=cluster)\n",
    "\n",
    "    # Calculate the bounding circle for the cluster\n",
    "    x_coords = [pos[node][0] for node in nodes_in_cluster]\n",
    "    y_coords = [pos[node][1] for node in nodes_in_cluster]\n",
    "    center_x = sum(x_coords) / len(x_coords)\n",
    "    center_y = sum(y_coords) / len(y_coords)\n",
    "    radius = max(\n",
    "        ((x - center_x)**2 + (y - center_y)**2)**0.5 \n",
    "        for x, y in zip(x_coords, y_coords)\n",
    "    ) + 0.1  # Add a margin\n",
    "    \n",
    "    # Draw the circle\n",
    "    #circle = plt.Circle((center_x, center_y), radius, color=color, fill=False, linewidth=2, alpha=0.5)\n",
    "    #plt.gca().add_artist(circle)\n",
    "\n",
    "    # Add the label near the circle\n",
    "    if p_values[counter] != 1:\n",
    "        plt.text(center_x + radius, center_y, f\"{cluster}\\n p-value: {round(p_values[counter], 3)}\", fontsize=8, color=color)\n",
    "    else:\n",
    "        plt.text(center_x + radius, center_y, f\"{cluster}\", fontsize=8, color=color)\n",
    "    counter += 1\n",
    "\n",
    "# Visualize the network\n",
    "plt.title(\"Network Visualization with Cluster Circles\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Netwulf Cluster Graph (interactive - without labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = construct_graph(cluster_data, debug_mode=False)\n",
    "#print(network)\n",
    "wulf.visualize(network)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
